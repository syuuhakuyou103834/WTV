# 批量刻蚀模拟异常值处理器
# 专门用于批量处理模式下的异常值检测和优化
# 与单片处理器分离，避免功能耦合

import numpy as np
import pandas as pd
import os
import logging
import time
from typing import Optional, Tuple, List, Dict, Any
from PyQt5.QtWidgets import QMessageBox, QDialog, QVBoxLayout, QLabel, QPushButton, QApplication, QProgressDialog
from PyQt5.QtCore import Qt, QTimer, pyqtSignal
import datetime

# 配置日志
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class OutlierBatchProcessor:
    """批量刻蚀模拟异常值处理器 - 专注于批量处理场景"""

    def __init__(self, main_window, uniformity_threshold):
        """
        初始化批量异常值处理器

        Args:
            main_window: 主窗口对象，用于状态更新
            uniformity_threshold: 均匀性阈值
        """
        self.main_window = main_window
        self.uniformity_threshold = uniformity_threshold

        # 批量处理状态
        self.batch_file_list = []  # 批量处理的文件列表
        self.current_batch_index = 0  # 当前处理的文件索引
        self.is_batch_processing = False  # 是否正在进行批量处理
        self.batch_start_time = None  # 批量处理开始时间
        self.batch_time_str = ""  # 批量处理时间戳
        self.lot_recipe_dir = ""  # Lot_Recipe文件夹路径

        # 模拟相关变量
        self.current_file_path = None  # 当前处理的文件路径
        self.current_file_name = None  # 当前处理的文件名
        self.current_output_dir = None  # 当前输出目录

        # 统计信息
        self.batch_results = []  # 批量处理结果列表
        self.batch_stats = {
            'total_files': 0,
            'processed_files': 0,
            'successful_files': 0,
            'failed_files': 0,
            'total_processing_time': 0.0,
            'average_uniformity': 0.0,
            'best_uniformity': float('inf'),
            'worst_uniformity': 0.0
        }

        logger.info(f"OutlierBatchProcessor 初始化完成，均匀性阈值: {uniformity_threshold}%")

    def set_batch_files(self, file_list):
        """
        设置批量处理的文件列表

        Args:
            file_list: 文件路径列表
        """
        self.batch_file_list = file_list
        self.current_batch_index = 0
        self.batch_stats['total_files'] = len(file_list)
        logger.info(f"设置批量处理文件列表，共 {len(file_list)} 个文件")

    def start_batch_processing(self, beam_file, processor, output_settings):
        """
        开始批量处理流程

        Args:
            beam_file: 离子束文件路径
            processor: 刻蚀处理器对象
            output_settings: 输出设置字典
        """
        if not self.batch_file_list:
            logger.error("批量处理文件列表为空")
            return False

        if not beam_file or not os.path.exists(beam_file):
            logger.error("离子束文件不存在")
            return False

        try:
            # 初始化批量处理状态
            self.is_batch_processing = True
            self.batch_start_time = time.time()
            self.batch_time_str = time.strftime("%Y%m%d-%H%M%S")
            self.current_batch_index = 0
            self.batch_results = []

            # 存储批量处理参数，避免在回调中丢失
            self._current_beam_file = beam_file
            self._current_processor = processor
            self._current_output_settings = output_settings

            # 创建Lot_Recipe文件夹
            first_file = self.batch_file_list[0]
            self.lot_recipe_dir = self._create_lot_recipe_folder(first_file)

            # 处理第一个文件
            return self._process_next_batch_file(beam_file, processor, output_settings)

        except Exception as e:
            logger.error(f"启动批量处理失败: {str(e)}")
            self.is_batch_processing = False
            return False

    def _create_lot_recipe_folder(self, first_file_path):
        """
        创建Lot_Recipe文件夹

        Args:
            first_file_path: 第一个文件的路径，用于确定文件夹位置

        Returns:
            str: Lot_Recipe文件夹路径
        """
        try:
            # 获取第一个文件所在目录
            file_dir = os.path.dirname(first_file_path)

            print(f"[DEBUG] 批量处理时间戳: {self.batch_time_str}")
            print(f"[DEBUG] 第一个文件路径: {first_file_path}")
            print(f"[DEBUG] 文件所在目录: {file_dir}")

            # 创建Lot_Recipe文件夹
            lot_recipe_dir = os.path.join(file_dir, f"{self.batch_time_str}_Lot_Recipe")
            print(f"[DEBUG] 准备创建Lot_Recipe文件夹: {lot_recipe_dir}")

            os.makedirs(lot_recipe_dir, exist_ok=True)

            # 验证文件夹是否成功创建
            if os.path.exists(lot_recipe_dir):
                print(f"[DEBUG] Lot_Recipe文件夹创建成功: {lot_recipe_dir}")
                print(f"[DEBUG] 文件夹权限: {oct(os.stat(lot_recipe_dir).st_mode)[-3:]}")
                return lot_recipe_dir
            else:
                print(f"[DEBUG] Lot_Recipe文件夹创建失败: {lot_recipe_dir}")
                return None

        except Exception as e:
            print(f"[DEBUG] 创建Lot_Recipe文件夹失败: {str(e)}")
            import traceback
            traceback.print_exc()
            return None

    def _process_next_batch_file(self, beam_file, processor, output_settings):
        """
        处理批量处理中的下一个文件

        Args:
            beam_file: 离子束文件路径
            processor: 刻蚀处理器对象
            output_settings: 输出设置字典

        Returns:
            bool: 是否成功开始处理
        """
        if self.current_batch_index >= len(self.batch_file_list):
            # 所有文件处理完成
            self._finish_batch_processing()
            return False

        current_file = self.batch_file_list[self.current_batch_index]
        file_name = os.path.basename(current_file)
        file_base_name = os.path.splitext(file_name)[0]
        file_dir = os.path.dirname(current_file)

        # 创建结果文件夹
        result_dir = os.path.join(file_dir, f"{file_base_name}_simulation_result")
        os.makedirs(result_dir, exist_ok=True)

        # 设置当前处理的文件信息
        self.current_file_path = current_file
        self.current_file_name = file_name
        self.current_output_dir = result_dir

        # 显示处理进度
        progress_msg = f"批量处理进度：{self.current_batch_index + 1}/{len(self.batch_file_list)}\n正在处理：{file_name}"
        self.main_window.update_status_message(progress_msg)

        logger.info(f"开始处理批量文件 {self.current_batch_index + 1}/{len(self.batch_file_list)}: {file_name}")

        # 调用外部接口启动单片批量处理
        # 主窗口就是MainWindow实例，直接调用其start_single_batch_processing方法
        if hasattr(self.main_window, 'start_single_batch_processing'):
            print(f"[DEBUG] OutlierBatchProcessor: 调用MainWindow.start_single_batch_processing")
            return self.main_window.start_single_batch_processing(
                current_file, result_dir, beam_file, processor,
                self._on_batch_simulation_completed
            )
        else:
            logger.error("无法访问主窗口的start_single_batch_processing方法")
            return False

    def _on_batch_simulation_completed(self, results, file_path, output_dir):
        """
        批量单片模拟完成处理回调

        Args:
            results: 模拟结果字典
            file_path: 处理的文件路径
            output_dir: 输出目录
        """
        try:
            # 修复：处理file_path为None的情况
            if file_path is None:
                # 从当前批量索引获取文件路径
                if self.current_batch_index < len(self.batch_file_list):
                    file_path = self.batch_file_list[self.current_batch_index]
                    logger.info(f"使用备用文件路径: {file_path}")
                else:
                    logger.error("无法确定当前处理的文件路径")
                    self._finish_batch_processing()
                    return

            file_name = os.path.basename(file_path)
            logger.info(f"批量单片模拟完成: {file_name}")

            # 统计处理结果
            self._collect_batch_result(results, file_name)

            # 备份Recipe文件到Lot_Recipe文件夹
            if output_dir:
                file_base_name = os.path.splitext(file_name)[0]
                self._backup_recipe_to_lot_folder(output_dir, file_base_name)
            else:
                logger.warning("输出目录为空，跳过Recipe备份")

            # 继续处理下一个文件
            self.current_batch_index += 1
            # 使用存储的参数继续处理下一个文件
            if hasattr(self, '_current_beam_file') and hasattr(self, '_current_processor') and hasattr(self, '_current_output_settings'):
                beam_file = self._current_beam_file
                processor = self._current_processor
                output_settings = self._current_output_settings
                self._process_next_batch_file(beam_file, processor, output_settings)
            else:
                logger.error("批量处理状态丢失，无法继续下一个文件")
                self._finish_batch_processing()

        except Exception as e:
            logger.error(f"处理批量单片完成结果时出错: {str(e)}")
            import traceback
            logger.error(f"详细错误: {traceback.format_exc()}")

            # 错误恢复：继续处理下一个文件
            self.current_batch_index += 1
            if hasattr(self, '_current_beam_file') and hasattr(self, '_current_processor') and hasattr(self, '_current_output_settings'):
                logger.info("尝试继续处理下一个文件")
                self._process_next_batch_file(self._current_beam_file, self._current_processor, self._current_output_settings)
            else:
                logger.error("批量处理状态丢失，结束批量处理")
                self._finish_batch_processing()

    def _collect_batch_result(self, results, file_name):
        """
        收集批量处理结果

        Args:
            results: 模拟结果字典
            file_name: 文件名
        """
        try:
            # 提取关键统计信息
            validated_stats = results.get('validated_thickness_stats', {})
            initial_stats = results.get('initial_thickness_stats', {})

            result_info = {
                'file_name': file_name,
                'success': True,
                'uniformity': validated_stats.get('uniformity', 0.0),
                'target_thickness': results.get('target_thickness', 0.0),
                'initial_thickness_stats': initial_stats,
                'validated_thickness_stats': validated_stats,
                'processing_time': time.time() - self.batch_start_time,
                'timestamp': datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }

            self.batch_results.append(result_info)

            # 更新统计信息
            self.batch_stats['processed_files'] += 1
            self.batch_stats['successful_files'] += 1

            uniformity = result_info['uniformity']
            self.batch_stats['average_uniformity'] += uniformity
            self.batch_stats['best_uniformity'] = min(self.batch_stats['best_uniformity'], uniformity)
            self.batch_stats['worst_uniformity'] = max(self.batch_stats['worst_uniformity'], uniformity)

            logger.info(f"收集批量结果: {file_name}, 均匀性: {uniformity:.2f}%")

        except Exception as e:
            logger.error(f"收集批量结果失败: {str(e)}")
            # 添加失败记录
            self.batch_results.append({
                'file_name': file_name,
                'success': False,
                'error': str(e),
                'timestamp': datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            })
            self.batch_stats['failed_files'] += 1

    def _backup_recipe_to_lot_folder(self, simulation_result_dir, file_name):
        """
        备份Recipe文件到Lot_Recipe文件夹
        优先备份倍速多次扫描Recipe，如果存在倍速Recipe则不备份原速度Recipe

        Args:
            simulation_result_dir: 模拟结果文件夹路径
            file_name: 原始文件名（不含扩展名）
        """
        try:
            print(f"[DEBUG] 开始备份Recipe: {file_name}")
            print(f"[DEBUG] Lot_Recipe文件夹路径: {self.lot_recipe_dir}")
            print(f"[DEBUG] 模拟结果文件夹路径: {simulation_result_dir}")

            if not self.lot_recipe_dir:
                print(f"[DEBUG] Lot_Recipe文件夹路径为空，跳过备份")
                return

            if not os.path.exists(self.lot_recipe_dir):
                print(f"[DEBUG] Lot_Recipe文件夹不存在，跳过备份: {self.lot_recipe_dir}")
                return

            if not os.path.exists(simulation_result_dir):
                print(f"[DEBUG] 模拟结果文件夹不存在，跳过备份: {simulation_result_dir}")
                return

            # 查找倍速Recipe文件（优先）
            base_name = file_name
            speed_recipes = []
            normal_recipes = []
            all_files = os.listdir(simulation_result_dir)

            print(f"[DEBUG] 在文件夹中查找Recipe文件: {simulation_result_dir}")
            print(f"[DEBUG] 所有文件: {[f for f in all_files if 'Recipe' in f]}")

            # 分类查找Recipe文件
            for filename in all_files:
                if "_Stage_Movement_Instruction_Recipe.csv" in filename:
                    if ("2_scan_" in filename or "3_scan_" in filename):
                        # 检查倍速Recipe是否包含base_name
                        if base_name in filename:
                            speed_recipes.append(filename)
                            print(f"[DEBUG] 找到倍速Recipe: {filename}")
                    elif filename.startswith(base_name) and not ("2_scan_" in filename or "3_scan_" in filename):
                        normal_recipes.append(filename)
                        print(f"[DEBUG] 找到普通Recipe: {filename}")

            print(f"[DEBUG] 倍速Recipe数量: {len(speed_recipes)}")
            print(f"[DEBUG] 普通Recipe数量: {len(normal_recipes)}")

            # 如果有倍速Recipe，只备份倍速Recipe，不备份原速度Recipe
            if speed_recipes:
                print(f"[DEBUG] 选择备份倍速Recipe: {speed_recipes}")
                for speed_recipe in speed_recipes:
                    src_path = os.path.join(simulation_result_dir, speed_recipe)
                    dst_path = os.path.join(self.lot_recipe_dir, speed_recipe)

                    if os.path.exists(src_path):
                        import shutil
                        shutil.copy2(src_path, dst_path)
                        print(f"[DEBUG] 成功备份倍速Recipe: {speed_recipe} -> {self.lot_recipe_dir}")
                    else:
                        print(f"[DEBUG] 倍速Recipe文件不存在: {src_path}")
                print(f"[DEBUG] {base_name} 存在倍速Recipe，已备份倍速Recipe，跳过原速度Recipe")
            elif normal_recipes:
                print(f"[DEBUG] 选择备份普通Recipe: {normal_recipes[0]}")
                filename = normal_recipes[0]
                src_path = os.path.join(simulation_result_dir, filename)
                dst_path = os.path.join(self.lot_recipe_dir, filename)

                if os.path.exists(src_path):
                    import shutil
                    shutil.copy2(src_path, dst_path)
                    print(f"[DEBUG] 成功备份普通Recipe: {filename} -> {self.lot_recipe_dir}")
                else:
                    print(f"[DEBUG] 普通Recipe文件不存在: {src_path}")
            else:
                print(f"[DEBUG] 未找到任何Recipe文件: {file_name}")

        except Exception as e:
            print(f"[DEBUG] 备份Recipe失败: {str(e)}")
            import traceback
            traceback.print_exc()

    def _finish_batch_processing(self):
        """完成批量处理"""
        try:
            # 计算最终统计
            if self.batch_stats['processed_files'] > 0:
                self.batch_stats['average_uniformity'] /= self.batch_stats['processed_files']

            total_processing_time = time.time() - self.batch_start_time
            self.batch_stats['total_processing_time'] = total_processing_time

            # 重置批量处理状态
            self.is_batch_processing = False

            # 生成批量处理汇总日志
            self._generate_batch_summary_log()

            # 通知主窗口批量处理完成
            if self.main_window:
                self.main_window.update_status_message(
                    f"批量处理完成: 共处理 {self.batch_stats['total_files']} 个文件，"
                    f"成功 {self.batch_stats['successful_files']} 个，"
                    f"失败 {self.batch_stats['failed_files']} 个，"
                    f"总用时 {total_processing_time:.1f} 秒"
                )

                if self.batch_stats['failed_files'] == 0:
                    QMessageBox.information(
                        None, "批量处理完成",
                        f"所有文件的处理已完成！\n\n"
                        f"总文件数: {self.batch_stats['total_files']}\n"
                        f"成功处理: {self.batch_stats['successful_files']}\n"
                        f"平均均匀性: {self.batch_stats['average_uniformity']:.2f}%\n"
                        f"总用时: {total_processing_time:.1f} 秒\n\n"
                        f"Log汇总文件已生成。"
                    )
                else:
                    QMessageBox.warning(
                        None, "批量处理完成（部分失败）",
                        f"批量处理完成，但有部分文件处理失败！\n\n"
                        f"总文件数: {self.batch_stats['total_files']}\n"
                        f"成功处理: {self.batch_stats['successful_files']}\n"
                        f"处理失败: {self.batch_stats['failed_files']}\n"
                        f"平均均匀性: {self.batch_stats['average_uniformity']:.2f}%\n"
                        f"总用时: {total_processing_time:.1f} 秒\n\n"
                        f"Log汇总文件已生成。"
                    )

            logger.info(f"批量处理完成: {self.batch_stats}")

        except Exception as e:
            logger.error(f"完成批量处理时出错: {str(e)}")
            if self.main_window:
                QMessageBox.warning(None, "批量处理完成（错误）",
                                 f"批量处理完成，但生成汇总时出错。\n\n错误信息: {str(e)}")

    def _generate_batch_summary_log(self):
        """生成批量处理汇总Log文件"""
        try:
            print(f"[DEBUG] 开始生成批量处理汇总Log文件")

            if not self.lot_recipe_dir:
                print(f"[DEBUG] Lot_Recipe文件夹不存在，无法生成汇总Log")
                return None

            # 生成Log文件名
            log_file_path = os.path.join(
                self.lot_recipe_dir,
                f"{self.batch_time_str}_Batch_Processing_Summary.log"
            )

            # 写入Log文件
            return self._write_batch_log_file(log_file_path)

        except Exception as e:
            print(f"[DEBUG] 生成批量处理Log汇总失败: {str(e)}")
            return None

    def _write_batch_log_file(self, log_file_path):
        """
        写入批量处理汇总Log文件

        Args:
            log_file_path: Log文件路径

        Returns:
            str: Log文件路径，失败时返回None
        """
        try:
            with open(log_file_path, 'w', encoding='utf-8') as f:
                # 写入文件头
                f.write("=" * 80 + "\n")
                f.write("批量刻蚀模拟处理汇总报告\n")
                f.write("=" * 80 + "\n\n")

                # 写入基本信息
                f.write(f"批量处理时间戳: {self.batch_time_str}\n")
                f.write(f"处理开始时间: {datetime.datetime.fromtimestamp(self.batch_start_time).strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"处理结束时间: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"总处理时间: {self.batch_stats['total_processing_time']:.2f} 秒\n\n")

                # 写入统计摘要
                f.write("处理统计摘要:\n")
                f.write("-" * 40 + "\n")
                f.write(f"总文件数: {self.batch_stats['total_files']}\n")
                f.write(f"成功处理: {self.batch_stats['successful_files']}\n")
                f.write(f"处理失败: {self.batch_stats['failed_files']}\n")
                f.write(f"成功率: {(self.batch_stats['successful_files'] / self.batch_stats['total_files'] * 100):.2f}%\n")
                f.write(f"平均均匀性: {self.batch_stats['average_uniformity']:.2f}%\n")
                f.write(f"最佳均匀性: {self.batch_stats['best_uniformity']:.2f}%\n")
                f.write(f"最差均匀性: {self.batch_stats['worst_uniformity']:.2f}%\n\n")

                # 写入文件列表
                f.write("处理文件详情:\n")
                f.write("-" * 40 + "\n")
                for i, result in enumerate(self.batch_results, 1):
                    if result['success']:
                        f.write(f"{i:2d}. {result['file_name']} - 成功 - 均匀性: {result['uniformity']:.2f}% - 时间: {result['timestamp']}\n")
                    else:
                        f.write(f"{i:2d}. {result['file_name']} - 失败 - 错误: {result.get('error', '未知错误')} - 时间: {result['timestamp']}\n")

                f.write("\n" + "=" * 80 + "\n")

            print(f"[DEBUG] 批量处理Log文件写入成功: {log_file_path}")
            return log_file_path

        except Exception as e:
            print(f"[DEBUG] 写入批量处理Log文件失败: {str(e)}")
            return None

    def get_batch_statistics(self):
        """
        获取批量处理统计信息

        Returns:
            dict: 批量处理统计信息
        """
        return {
            'total_files': self.batch_stats['total_files'],
            'processed_files': self.batch_stats['processed_files'],
            'successful_files': self.batch_stats['successful_files'],
            'failed_files': self.batch_stats['failed_files'],
            'total_processing_time': self.batch_stats['total_processing_time'],
            'average_uniformity': self.batch_stats['average_uniformity'],
            'best_uniformity': self.batch_stats['best_uniformity'],
            'worst_uniformity': self.batch_stats['worst_uniformity'],
            'is_processing': self.is_batch_processing,
            'current_file': self.current_file_name,
            'current_index': self.current_batch_index
        }

    def cancel_batch_processing(self):
        """取消批量处理"""
        if self.is_batch_processing:
            self.is_batch_processing = False
            logger.info("批量处理已取消")
            if self.main_window:
                self.main_window.update_status_message("批量处理已取消")
            return True
        return False

    def is_processing(self):
        """
        检查是否正在批量处理

        Returns:
            bool: 是否正在处理
        """
        return self.is_batch_processing

    def get_progress_info(self):
        """
        获取批量处理进度信息

        Returns:
            dict: 进度信息
        """
        return {
            'total_files': len(self.batch_file_list),
            'current_index': self.current_batch_index,
            'current_file': self.current_file_name,
            'is_processing': self.is_batch_processing
        }

    def on_batch_simulation_error(self, error_message):
        """
        批量单片模拟错误处理（提供给EtchingSimulationUI调用）

        Args:
            error_message: 错误消息
        """
        try:
            # 获取当前处理的文件名
            if self.current_batch_index < len(self.batch_file_list):
                current_file_name = os.path.basename(self.batch_file_list[self.current_batch_index])
                logger.error(f"批量处理文件 {current_file_name} 时发生错误: {error_message}")
            else:
                logger.error(f"批量处理过程中发生错误: {error_message}")

            # 继续处理下一个文件
            self.current_batch_index += 1
            self._process_next_batch_file(None, None, None)  # 这里需要传入之前保存的beam_file和processor

        except Exception as e:
            logger.error(f"处理批量模拟错误时发生异常: {str(e)}")
            # 强制完成批量处理
            self._finish_batch_processing()